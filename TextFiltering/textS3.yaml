AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instance
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: Can contain only ASCII characters.

Mappings:
  RegionAMI:
      us-east-1:
        HVM64: ami-00dc79254d0461090
      us-west-2:
        HVM64: ami-0a85857bfc5345c38
      eu-west-1:
        HVM64: ami-040ba9174949f6de4
      # us-east-1:
      #   HVM64: ami-00dc79254d0461090
      # us-west-2:
      #   HVM64: ami-0a85857bfc5345c38
      # eu-west-1:
      #   HVM64: ami-040ba9174949f6de4

Resources:
  MSKVPCStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: "https://aws-streaming-artifacts.s3.amazonaws.com/msk-lab-resources/MSKLabs/MSKPrivateVPCOnly.yml"

  KafkaClientInstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH and prometheus access from BastionHostSecurityGroup
      VpcId: !GetAtt MSKVPCStack.Outputs.VPCId
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 9092
        ToPort: 9092
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 3500
        ToPort: 3500
        CidrIp: 10.0.0.0/24
      - IpProtocol: tcp
        FromPort: 3600
        ToPort: 3600
        CidrIp: 10.0.0.0/24
      - IpProtocol: tcp
        FromPort: 3800
        ToPort: 3800
        CidrIp: 10.0.0.0/24
      - IpProtocol: tcp
        FromPort: 3900
        ToPort: 3900
        CidrIp: 10.0.0.0/24
      - IpProtocol: tcp
        FromPort: 61617
        ToPort: 61617
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 2888
        ToPort: 3888
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 8162
        ToPort: 8162
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 2181
        ToPort: 2181
        CidrIp: 0.0.0.0/0
      - IpProtocol: tcp
        FromPort: 9094
        ToPort: 9094
        CidrIp: 0.0.0.0/0

  KafkaClientInstanceSecurityGroup8081:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: KafkaClientInstanceSecurityGroup
    Properties:
      Description: Enable access to Schema Registry inside the KafkaClientInstanceSecurityGroup
      GroupId: !GetAtt KafkaClientInstanceSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 8081
      ToPort: 8081
      SourceSecurityGroupId: !GetAtt KafkaClientInstanceSecurityGroup.GroupId

  KafkaClientInstanceSecurityGroup8083:
    Type: AWS::EC2::SecurityGroupIngress
    DependsOn: KafkaClientInstanceSecurityGroup
    Properties:
      Description: Enable access to Kafka Connect inside the KafkaClientInstanceSecurityGroup
      GroupId: !GetAtt KafkaClientInstanceSecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 8083
      ToPort: 8083
      SourceSecurityGroupId: !GetAtt KafkaClientInstanceSecurityGroup.GroupId

  Cloud9EC2Bastion:
    Type: AWS::Cloud9::EnvironmentEC2
    Properties: 
      AutomaticStopTimeMinutes: 600
      Description: "Cloud9 EC2 environment"
      InstanceType: m5.large
      Name: !Sub "${AWS::StackName}-Cloud9EC2Bastion"
      SubnetId: !GetAtt MSKVPCStack.Outputs.PublicSubnetOne
      Tags: 
        - Key: 'Purpose'
          Value: 'Cloud9EC2BastionHostInstance'

  KafkaClientEC2Instance1:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: m5.large
      KeyName: !Ref 'KeyName'
      IamInstanceProfile: !Ref EC2InstanceProfile
      AvailabilityZone: 
        Fn::Select:
         - 0
         - Fn::GetAZs: {Ref: 'AWS::Region'}
      SubnetId: !GetAtt MSKVPCStack.Outputs.PublicSubnetOne
      SecurityGroupIds: [!GetAtt KafkaClientInstanceSecurityGroup.GroupId]
      ImageId: !FindInMap ['RegionAMI', !Ref 'AWS::Region', 'HVM64']
      Tags:
        - Key: 'Name'
          Value: !Sub ${AWS::StackName}-KafkaClientInstance1
      UserData: 
        Fn::Base64: 
          !Sub |
            #!/bin/bash
            yum update -y
            yum install python3.7 -y
            yum install java-1.8.0-openjdk-devel -y
            yum install nmap-ncat -y
            yum install git -y
            yum erase awscli -y
            yum install jq -y
            yum install maven -y
            amazon-linux-extras install docker -y
            service docker start
            usermod -a -G docker ec2-user

            cd /home/ec2-user
            wget https://bootstrap.pypa.io/get-pip.py
            su -c "python3.7 get-pip.py --user" -s /bin/sh ec2-user
            su -c "/home/ec2-user/.local/bin/pip3 install boto3 --user" -s /bin/sh ec2-user
            su -c "/home/ec2-user/.local/bin/pip3 install awscli --user" -s /bin/sh ec2-user
            su -c "/home/ec2-user/.local/bin/pip3 install kafka-python --user" -s /bin/sh ec2-user

            # install AWS CLI 2 - access with aws2
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            ./aws/install -b /usr/local/bin/aws2
            su -c "ln -s /usr/local/bin/aws2/aws ~/.local/bin/aws2" -s /bin/sh ec2-user

            # Create dirs, get Apache Kafka 2.3.1, 2.4.0 and unpack it
            su -c "mkdir -p kafka231 kafka241 confluent" -s /bin/sh ec2-user
            cd kafka231
            su -c "wget https://archive.apache.org/dist/kafka/2.3.1/kafka_2.12-2.3.1.tgz" -s /bin/sh ec2-user
            su -c "tar -xzf kafka_2.12-2.3.1.tgz --strip 1" -s /bin/sh ec2-user
           
            cd /home/ec2-user
            ln -s /home/ec2-user/kafka241 /home/ec2-user/kafka
            cd kafka241
            su -c "wget http://archive.apache.org/dist/kafka/2.4.1/kafka_2.12-2.4.1.tgz" -s /bin/sh ec2-user
            su -c "tar -xzf kafka_2.12-2.4.1.tgz --strip 1" -s /bin/sh ec2-user

            # Get Confluent Community and unpack it
            cd /home/ec2-user
            cd confluent
            su -c "wget http://packages.confluent.io/archive/5.4/confluent-community-5.4.1-2.12.tar.gz" -s /bin/sh ec2-user
            su -c "tar -xzf confluent-community-5.4.1-2.12.tar.gz --strip 1" -s /bin/sh ec2-user
            
            # Initialize the Kafka cert trust store
            su -c 'find /usr/lib/jvm/ -name "cacerts" -exec cp {} /tmp/kafka.client.truststore.jks \;' -s /bin/sh ec2-user

            # Setup prometheus agent
            cd /home/ec2-user
            su -c "mkdir prometheus" -s /bin/sh ec2-user
            cd prometheus
            su -c "wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.13.0/jmx_prometheus_javaagent-0.13.0.jar" -s /bin/sh ec2-user

            # Setup Grafana
            # cd /home/ec2-user
            # su -c "mkdir grafana" -s /bin/sh ec2-user
            # cd grafana
            # su -c "wget https://dl.grafana.com/oss/release/grafana-7.0.1-1.x86_64.rpm" -s /bin/sh ec2-user
            # yum install grafana-7.0.1-1.x86_64.rpm -y
            # usermod -a -G grafana ec2-user
            # systemctl status grafana-server.service

            # Copy files from S3
            cd /tmp
            su -c "mkdir -p kafka" -s /bin/sh ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/producer.properties_msk /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/consumer.properties /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/KafkaClickstreamClient-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/KafkaClickstreamConsumer-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/CustomMM2ReplicationPolicy-1.0-SNAPSHOT.jar /home/ec2-user/confluent/share/java/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/MM2GroupOffsetSync-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/MSKLabs/schema-registry-ssl/schema-registry.properties /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/generatePropertiesFiles.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/generateStartupFile.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/AuthMSK-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/connect-distributed.properties /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/kafka-consumer-python.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/setup-env.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/GlobalSeqNo.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-msc.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-hbc.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-cpc.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-cpc-cust-repl-policy.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-msc-cust-repl-policy.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/kafka-connect.yml /home/ec2-user/prometheus" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/kafka-producer-consumer.yml /home/ec2-user/prometheus" -l ec2-user

            # Setup unit in systemd for Schema Registry
            echo -n "
            [Unit]
            Description=Confluent Schema Registry
            After=network.target

            [Service]
            Type=simple
            User=ec2-user
            ExecStart=/bin/sh -c '/home/ec2-user/confluent/bin/schema-registry-start /tmp/kafka/schema-registry.properties > /tmp/kafka/schema-registry.log 2>&1'
            ExecStop=/home/ec2-user/confluent/bin/schema-registry-stop
            Restart=on-abnormal

            [Install]
            WantedBy=multi-user.target" > /etc/systemd/system/confluent-schema-registry.service

            # Setup unit in systemd for Kafka Connect
            echo -n "
            [Unit]
            Description=Kafka Connect
            After=network.target

            [Service]
            Type=simple
            User=ec2-user
            Environment='KAFKA_OPTS=-javaagent:/home/ec2-user/prometheus/jmx_prometheus_javaagent-0.13.0.jar=3600:/home/ec2-user/prometheus/kafka-connect.yml'
            ExecStart=/bin/sh -c '/home/ec2-user/confluent/bin/connect-distributed /tmp/kafka/connect-distributed.properties > /tmp/kafka/kafka-connect.log 2>&1'
            Restart=on-abnormal

            [Install]
            WantedBy=multi-user.target" > /etc/systemd/system/kafka-connect.service

            # Setup unit in systemd for Prometheus
            # echo -n "
            # [Unit]
            # Description=Prometheus
            # After=network.target

            # [Service]
            # Type=simple
            # User=root
            # ExecStart=/bin/sh -c '/home/ec2-user/prometheus/prometheus/prometheus --config.file=/home/ec2-user/prometheus/prometheus/prometheus.yml > /tmp/kafka/prometheus.log 2>&1'
            # Restart=on-abnormal

            # [Install]
            # WantedBy=multi-user.target" > /etc/systemd/system/prometheus.service

            #setup bash env
            su -c "echo 'export PS1=\"KafkaClientEC2Instance1 [\u@\h \W\\]$ \"' >> /home/ec2-user/.bash_profile" -s /bin/sh ec2-user
            su -c "echo '[ -f /tmp/kafka/setup_env ] && . /tmp/kafka/setup_env' >> /home/ec2-user/.bash_profile" -s /bin/sh ec2-user

  KafkaClientEC2Instance2:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: m5.large
      KeyName: !Ref 'KeyName'
      IamInstanceProfile: !Ref EC2InstanceProfile
      AvailabilityZone: 
        Fn::Select:
         - 1
         - Fn::GetAZs: {Ref: 'AWS::Region'}
      SubnetId: !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKTwo
      SecurityGroupIds: [!GetAtt KafkaClientInstanceSecurityGroup.GroupId]
      ImageId: !FindInMap ['RegionAMI', !Ref 'AWS::Region', 'HVM64']
      Tags:
        - Key: 'Name'
          Value: !Sub ${AWS::StackName}-KafkaClientInstance2
      UserData: 
        Fn::Base64: 
          !Sub | 
            #!/bin/bash
            yum update -y
            yum install python3.7 -y
            yum install java-1.8.0-openjdk-devel -y
            yum install nmap-ncat -y
            yum install git -y
            yum erase awscli -y
            yum install jq -y
            yum install maven -y
            amazon-linux-extras install docker -y
            service docker start
            usermod -a -G docker ec2-user

            cd /home/ec2-user
            wget https://bootstrap.pypa.io/get-pip.py
            su -c "python3.7 get-pip.py --user" -s /bin/sh ec2-user
            su -c "/home/ec2-user/.local/bin/pip3 install boto3 --user" -s /bin/sh ec2-user
            su -c "/home/ec2-user/.local/bin/pip3 install awscli --user" -s /bin/sh ec2-user
            su -c "/home/ec2-user/.local/bin/pip3 install kafka-python --user" -s /bin/sh ec2-user

            # install AWS CLI 2 - access with aws2
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            ./aws/install -b /usr/local/bin/aws2
            su -c "ln -s /usr/local/bin/aws2/aws ~/.local/bin/aws2" -s /bin/sh ec2-user

            # Create dirs, get Apache Kafka 2.3.1, 2.4.0 and unpack it
            su -c "mkdir -p kafka231 kafka241 confluent" -s /bin/sh ec2-user
            cd kafka231
            su -c "wget https://archive.apache.org/dist/kafka/2.3.1/kafka_2.12-2.3.1.tgz" -s /bin/sh ec2-user
            su -c "tar -xzf kafka_2.12-2.3.1.tgz --strip 1" -s /bin/sh ec2-user

            cd /home/ec2-user
            ln -s /home/ec2-user/kafka241 /home/ec2-user/kafka
            cd kafka241
            su -c "wget http://archive.apache.org/dist/kafka/2.4.1/kafka_2.12-2.4.1.tgz" -s /bin/sh ec2-user
            su -c "tar -xzf kafka_2.12-2.4.1.tgz --strip 1" -s /bin/sh ec2-user

            # Get Confluent Community and unpack it
            cd /home/ec2-user
            cd confluent
            su -c "wget http://packages.confluent.io/archive/5.4/confluent-community-5.4.1-2.12.tar.gz" -s /bin/sh ec2-user
            su -c "tar -xzf confluent-community-5.4.1-2.12.tar.gz --strip 1" -s /bin/sh ec2-user
            
            # Initialize the Kafka cert trust store
            su -c 'find /usr/lib/jvm/ -name "cacerts" -exec cp {} /tmp/kafka.client.truststore.jks \;' -s /bin/sh ec2-user

            # Setup prometheus agent
            cd /home/ec2-user
            su -c "mkdir prometheus" -s /bin/sh ec2-user
            cd prometheus
            su -c "wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.13.0/jmx_prometheus_javaagent-0.13.0.jar" -s /bin/sh ec2-user
            # su -c "wget https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-connect.yml" -s /bin/sh ec2-user

            # # Setup Grafana
            # cd /home/ec2-user
            # su -c "mkdir grafana" -s /bin/sh ec2-user
            # cd grafana
            # su -c "wget https://dl.grafana.com/oss/release/grafana-7.0.1-1.x86_64.rpm" -s /bin/sh ec2-user
            # yum install grafana-7.0.1-1.x86_64.rpm -y
            # usermod -a -G grafana ec2-user
            # systemctl status grafana-server.service

            # Copy files from S3
            cd /tmp
            su -c "mkdir -p kafka" -s /bin/sh ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/producer.properties_msk /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/consumer.properties /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/KafkaClickstreamClient-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/KafkaClickstreamConsumer-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/CustomMM2ReplicationPolicy-1.0-SNAPSHOT.jar /home/ec2-user/confluent/share/java/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/jars/MM2GroupOffsetSync-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/MSKLabs/schema-registry-ssl/schema-registry.properties /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/generatePropertiesFiles.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/generateStartupFile.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/AuthMSK-1.0-SNAPSHOT.jar /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/connect-distributed.properties /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/kafka-consumer-python.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/setup-env.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/GlobalSeqNo.py /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-msc.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-hbc.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-cpc.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-cpc-cust-repl-policy.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/mm2-msc-cust-repl-policy.json /tmp/kafka" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/kafka-connect.yml /home/ec2-user/prometheus" -l ec2-user
            su -c "aws s3 cp s3://aws-streaming-artifacts/msk-lab-resources/kafka-producer-consumer.yml /home/ec2-user/prometheus" -l ec2-user

            # Setup unit in systemd for Schema Registry
            echo -n "
            [Unit]
            Description=Confluent Schema Registry
            After=network.target

            [Service]
            Type=simple
            User=ec2-user
            ExecStart=/bin/sh -c '/home/ec2-user/confluent/bin/schema-registry-start /tmp/kafka/schema-registry.properties > /tmp/kafka/schema-registry.log 2>&1'
            ExecStop=/home/ec2-user/confluent/bin/schema-registry-stop
            Restart=on-abnormal

            [Install]
            WantedBy=multi-user.target" > /etc/systemd/system/confluent-schema-registry.service

            # Setup unit in systemd for Kafka Connect
            echo -n "
            [Unit]
            Description=Kafka Connect
            After=network.target

            [Service]
            Type=simple
            User=ec2-user
            Environment='KAFKA_OPTS=-javaagent:/home/ec2-user/prometheus/jmx_prometheus_javaagent-0.13.0.jar=3500:/home/ec2-user/prometheus/kafka-connect.yml'
            ExecStart=/bin/sh -c '/home/ec2-user/confluent/bin/connect-distributed /tmp/kafka/connect-distributed.properties > /tmp/kafka/kafka-connect.log 2>&1'
            Restart=on-abnormal

            [Install]
            WantedBy=multi-user.target" > /etc/systemd/system/kafka-connect.service

            # Setup unit in systemd for Prometheus
            # echo -n "
            # [Unit]
            # Description=Prometheus
            # After=network.target

            # [Service]
            # Type=simple
            # User=root
            # ExecStart=/bin/sh -c '/home/ec2-user/prometheus/prometheus/prometheus --config.file=/home/ec2-user/prometheus/prometheus/prometheus.yml > /tmp/kafka/prometheus.log 2>&1'
            # Restart=on-abnormal

            # [Install]
            # WantedBy=multi-user.target" > /etc/systemd/system/prometheus.service

            # #setup bash env
            su -c "echo 'export PS1=\"KafkaClientEC2Instance2 [\u@\h \W\\]$ \"' >> /home/ec2-user/.bash_profile" -s /bin/sh ec2-user
            su -c "echo '[ -f /tmp/kafka/setup_env ] && . /tmp/kafka/setup_env' >> /home/ec2-user/.bash_profile" -s /bin/sh ec2-user
            

  EC2Role: 
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: 
        Version: 2012-10-17
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: 'sts:AssumeRole'
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonMSKFullAccess
        - arn:aws:iam::aws:policy/AWSCloudFormationReadOnlyAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/AWSCertificateManagerPrivateCAFullAccess
  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties: 
      InstanceProfileName: !Join
                            - '-'
                            - - 'EC2MMMSKCFProfile'
                              - !Ref 'AWS::StackName'
      Roles:
        - !Ref EC2Role

  S3Bucket:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Retain
    Properties:
      BucketName: myfilterbucket
  MyLambda:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.8
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      Code:
        ZipFile: |
            import json
            import boto3
            import base64

            def filterString():

              global file_content
              s4 = boto3.client("s3")


              filename = 'filter.txt'
              print("Filename: ", filename)

   
              try:
                fileObj = s4.get_object(Bucket = "myfilterbucket", Key=filename);
              except:
                print("filter.txt Does Not exists in s3 bucket filters")
                file_content = None
                return None
    
              file_content = fileObj["Body"].read().decode('utf-8')

              #now i need this in my package to filter actually json message coming in 
              print("Here is filters: ", file_content)

              return file_content

            file_content = filterString()


            def messageHand(message):
              global file_content
              wordlist = file_content.split(",")

              
              for word in wordlist:
              if word not in message:
                print("MESSAGE FILTERED OUT")
                return 0

              print("Message Passed All Filters: " + message)

              return 0

  
            def process(message):
              global file_content
              if not file_content:
                if file_content is None:
                  print("No Filter File\n")
                print("NO FILTER TEXT GIVEN. MESSAGE: ", message)
                return 0
              messageHand(message)
              return 0
        
    
            def lambda_handler(event, context):
    
              global file_content
    
              #IMPORTANT - NOTE WHEN OUR CACHE FALLS OFF AND LAMBDA REINITIALIZES 
              #.BSS, .TEXT, AND .DATA SECTIONS WE WILL AUTOMATICALLY GET THE SCHEMA FROM
              #S3
              try:
                if event['Records'][0]['eventSource'] == 'aws:s3':
                  print("Filter Text Before: ", file_content)
                  if event['Records'][0]['eventName'] == 'ObjectRemoved:Delete':
                    file_content = None
                  else:
                    filterString()
                  print("Filter Text After: ", file_content)
                  return
              except:
                pass

              records = ''

              if event['eventSource'] == 'aws:mq':
                records = event['messages']
                for index, value in enumerate(records): 
                  payload=base64.b64decode(records[index]['data'])

                  message = str(payload, 'UTF-8')
                  process(message)

              else:
                records = event['records']
                for topic in records:
                  #topic
                  for index, value in enumerate(records[topic]):
                    payload=base64.b64decode(records[topic][index]['value'])

                    message = str(payload, 'UTF-8')
                    process(message)

              return event

      Description: Invoke a function during stack creation.
      TracingConfig:
        Mode: Active
      VpcConfig:
        SecurityGroupIds:
          - !GetAtt KafkaClientInstanceSecurityGroup.GroupId
        SubnetIds:
          - !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKOne
          - !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKTwo
      
  
  
  MSKCluster:
    Type: 'AWS::MSK::Cluster'
    Properties:
      BrokerNodeGroupInfo:
        ClientSubnets:
          - !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKOne
          - !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKTwo
          - !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKThree
        InstanceType: kafka.m5.large
        SecurityGroups:
          - !GetAtt 
            - KafkaClientInstanceSecurityGroup
            - GroupId
        StorageInfo:
          EBSStorageInfo:
            VolumeSize: 2000
      ClusterName: MSKCluster
      EncryptionInfo:
        EncryptionInTransit:
          ClientBroker: TLS
          InCluster: true
      EnhancedMonitoring: PER_TOPIC_PER_BROKER
      KafkaVersion: 2.2.1
      NumberOfBrokerNodes: 3

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: 
        Version: 2012-10-17
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambda_FullAccess
        - arn:aws:iam::aws:policy/AmazonMSKFullAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/AmazonMQFullAccess
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/SecretsManagerReadWrite
  mybroker:
    Type: 'AWS::AmazonMQ::Broker'
    Properties:
      AutoMinorVersionUpgrade: 'false'
      BrokerName: mybroker
      DeploymentMode: SINGLE_INSTANCE
      EngineType: ActiveMQ
      EngineVersion: 5.15.0
      HostInstanceType: mq.m5.xlarge
      PubliclyAccessible: 'true'
      SecurityGroups:
        - !GetAtt 
            - KafkaClientInstanceSecurityGroup
            - GroupId
      SubnetIds:
        - !GetAtt MSKVPCStack.Outputs.PublicSubnetOne
      Users:
        - ConsoleAccess: 'true'
          Groups:
            - MyGroup
          Password: erronamqpassword
          Username: erronamq
Outputs:
  VPCId: 
    Description: The ID of the VPC created
    Value: !GetAtt MSKVPCStack.Outputs.VPCId
  PublicSubnetOne: 
    Description: The name of the public subnet created
    Value: !GetAtt MSKVPCStack.Outputs.PublicSubnetOne
  PrivateSubnetMSKOne: 
    Description: The ID of private subnet one created
    Value: !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKOne
  PrivateSubnetMSKTwo: 
    Description: The ID of private subnet two created
    Value: !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKTwo
  PrivateSubnetMSKThree: 
    Description: The ID of private subnet three created
    Value: !GetAtt MSKVPCStack.Outputs.PrivateSubnetMSKThree
  KafkaClientEC2Instance1PrivateDNS:
    Description: The Public DNS for the EC2 instance1
    Value: !GetAtt KafkaClientEC2Instance1.PrivateDnsName
  KafkaClientEC2Instance2PrivateDNS:
    Description: The Public DNS for the EC2 instance2
    Value: !GetAtt KafkaClientEC2Instance2.PrivateDnsName
  SSHKafkaClientEC2Instance1:
    Description: SSH command for Kafka the EC2 instance1
    Value: !Sub ssh -A ec2-user@${KafkaClientEC2Instance1.PrivateDnsName}
    Export:
      Name: !Sub "${AWS::StackName}-SSHKafkaClientEC2Instance1"
  SSHKafkaClientEC2Instance2:
    Description: SSH command for Kafka the EC2 instance2
    Value: !Sub ssh -A ec2-user@${KafkaClientEC2Instance2.PrivateDnsName}
    Export:
      Name: !Sub "${AWS::StackName}-SSHKafkaClientEC2Instance2"
  KafkaClientEC2InstanceSecurityGroupId:
    Description: The security group id for the EC2 instance
    Value: !GetAtt KafkaClientInstanceSecurityGroup.GroupId
    Export:
      Name: !Sub "${AWS::StackName}-KafkaClientEC2InstanceSecurityGroupId"
  SchemaRegistryKafkaClientEC2Instance1Url:
    Description: The url for the Schema Registry
    Value: !Sub http://${KafkaClientEC2Instance1.PrivateDnsName}:8081
    Export:
      Name: !Sub "${AWS::StackName}-SchemaRegistryKafkaClientEC2Instance1Url"
  SchemaRegistryKafkaClientEC2Instance2Url:
    Description: The url for the Schema Registry
    Value: !Sub http://${KafkaClientEC2Instance2.PrivateDnsName}:8081
    Export:
      Name: !Sub "${AWS::StackName}-SchemaRegistryKafkaClientEC2Instance2Url"
  VPCStackName:
    Description: The name of the VPC Stack
    Value: !GetAtt MSKVPCStack.Outputs.VPCStackName

